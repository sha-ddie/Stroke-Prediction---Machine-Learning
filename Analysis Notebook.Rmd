---
title: "Stroke Prediction -Machine Learning"
author: "Mulei Mutuku"
date: "2024-02-13"
output: pdf_document
---

# Stroke Prediction

# Introduction


## Main Objective


## Specific Objectives


## Metric Of Success


# Data Understanding
The data used contains 12 features and 5,110 rows/ observations. The data entails, 12 stroke related factors collected from 5,110 individuals, in order to evaluate the relationships between experiencing a stroke and other features, identifying factors that could influence stroke risk. The feature include; 
- **id**: unique identifier
- **gender**: "Male", "Female" or "Other"
- **age**: age of the patient
- **hypertension**: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension
- **heart_disease**: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease.
- **ever_married**: "No" or "Yes".
- **work_type**: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed".
- **Residence_type**: "Rural" or "Urban".
- **avg_glucose_level**: average glucose level in blood (Normally should be 70-140 mg/dl).
- **bmi**: body mass index (Normally should be 18-25),
- **smoking_status**: "formerly smoked", "never smoked", "smokes" or "Unknown".
- **stroke**: 1 if the patient had a stroke or 0 if not.

Loading all the required libraries, used in analysis.
```{r}
library(dplyr)
library(ggplot2)
library(rsample)
library(yardstick)
library(stringr)
```

Importing the dataset, and previewing it.
```{r}
# loading data set into df variable
df = read.csv("./data/healthcare-dataset-stroke-data.csv")


# previewing data set
paste0("Rows: ",dim(df)[1]," Columns: ", dim(df)[2])
head(df)
```

```{r}
# viewing data
skimr::skim(df)

# summary statistics
summary(df)
```
The data set has 12 columns with 6 being of `character` type and 6 `numeric`.

Based on the summary statistic, the following observations are made;
1. The `age` and `avg_glucose_level` have outliers or they have skewed distributions.
2. The `age` column has a minimum age of 0 which is not possible.
3. The `bmi` column has character type and should be `numeric`.

# Data Preparation/ Cleaning
In this section, we will perform data cleaning to prepare the data set for analysis, the various data cleaning methods that are to be used will be;
- Data type conversion
- Checking for Missing values
- Checking and removing duplicates
- Removing Unnecessary columns
- Checking for Missing values Placeholder

From the summary statistics, we saw ages of zero, we then assign 1 where age is less than 1.
```{r}
# changing the minimum value of age column
df$age[df$age<1] = 1
```

```{r}
# Data type conversion for bmi column into numeric
class(df$bmi)

df$bmi = as.numeric(df$bmi)
```

```{r}
# checking for missing values or NAs
sapply(names(df), function(x){sum(is.na(df[[x]]))})
paste0("NAs: ",sum(is.na(df)))

# distribution of bmi column
summary(df$bmi)
```
From the summary statistics of the `bmi` column, the mean and median are approximately equal, implying a normal distribution. Therefore we will perform **mean** imputation to deal with Na's in `bmi` column.
```{r}
# Dealing with Na's in bmi column
df$bmi[ is.na(df$bmi)] = mean(df$bmi, na.rm = TRUE)
```

```{r}
# checking for duplicates
sum(duplicated(df))
```
Since the `id` column has no influence in our analysis, we remove it from out data. 
```{r}
#removing id column
#df = df[-1]
df= df %>% select(-id)
```

We will, identify the unique values of the categorical columns to investigate whether there are missing values represented by place holders. 
```{r}
# extracting only the categorical columns
cat_cols = df %>% select(where(is.character)) %>% colnames()

#checking the unique values in categorical variables for place holders
sapply(cat_cols, function(x){
                    y=df[[x]]
                    paste( unique(y))
        })
```
There are no placeholders in any of the columns, but the "Other" category in gender should imputed or removed.
```{r}
# removing the "Other" category in gender
table(df$gender)

# since the mode of the gender  column is female we assign `other` as female
df$gender[df$gender=="Other"] = "Female"

```
since the `mode` of the `gender`  column is female we assign `other` as female so as not to influence the distribution of the gender column.

From the `work_type` and `smoking_status` columns, we identify categories that have spaces and "-" between them, this will later be problematic during dummying and modeling. Therefore, we replace all the spaces and "-" between words with "_" symbol.

```{r}
# cleaning the text work_type and smoking_status columns 
df = df %>%
      mutate(work_type = str_replace_all(work_type, "[-| ]", "_"), 
            smoking_status = str_replace_all(smoking_status, "[-| ]", "_") )

# confirming
df %>%
  select(work_type, smoking_status) %>% unique()
```



# Eploratory Data Analysis
Examining the patterns and trends in the individual attributes through visualizations.
First, we will convert all the binary columns into factors for better visualizations 
```{r}
eda = df %>% 
        mutate( hypertension = ifelse(hypertension==1,"Yes","No"),
                heart_disease = ifelse(heart_disease==1,"Yes","No"),
                stroke= ifelse(stroke==1,"Yes","No"))
```


Plotting the value counts of the character values of their unique values
```{r}
# using sapply function to plot their distribution
cat_ = eda %>% select(where(is.character)) %>% colnames()

sapply(cat_, function(x){
  y= eda[[x]]
  plot =ggplot(data=eda, aes(x=y))+
    geom_bar(fill= "#436850")+
    geom_label( aes(label= after_stat(count)),
               stat="count")+
    labs(title=paste0("BarPlot of ",str_to_title(x)," Column"),
         y= "Counts",
         x= paste0(x))
  print(plot)
})
```

From the plots above, the following observations are made;
1.
2.


Plotting the relationships between our dependent variable ,`stroke`, and the character columns.
```{r}
#
sapply(cat_, function(x){
  y= eda[[x]]
  plot = ggplot(data=eda, aes(x= stroke, fill= y ))+
            geom_bar( position="dodge")+
            geom_label( 
              aes(label= paste0(round((after_stat(count)/5110)*100,0),"%")),
                  stat="count",
                  position = position_dodge(1), 
                  hjust=0.5,
                  vjust=-0.05)+
           labs(
                title=paste0(" Distribution of ",str_to_title(x),
                             " vs Stroke Column"),
                 y= "Counts",
                 x= "Stroke",
                fill =paste0(x))
        
  print(plot)
})
```

From the plots above, the following observations are made;
1.
2.



Plotting the distributions of numeric variables
```{r}
# extracting the numeric column
num_cols = eda %>% select(where(is.numeric)) %>% colnames()

# plotting their distribution
sapply(num_cols, function(x){
  y= eda[[x]]
  plot = ggplot(data=eda, aes(x=y))+
            geom_histogram(fill="#436850",bins=20)+
            labs(title=paste0("Histogram of ",str_to_title(x)," Column"),
                 y= "Frequency",
                 x= paste0(x))
  
  print(plot)
})
```

From the plots above, the following observations are made;
1.
2.




Plotting the realationship beyween numeric columns and `stroke` column
```{r}
# relationship between numeric columns and stroke
sapply(num_cols, function(x){
  y= eda[[x]]
  plot = ggplot(data=eda, aes(x=as.factor(stroke), y= y))+
          geom_boxplot()+
          labs(
          title=paste0("Boxplot of ",str_to_title(x)," Against Stroke Column"),
                 y= paste0(x),
                 x=  "Stroke")
  
  print(plot)
})
```

From the plots above, the following observations are made;
1.
2.



```{r}
library(GGally)

eda %>%
  select(gender, age, bmi,stroke) %>% 
  ggpairs(mapping = aes(color=stroke))
```

```{r}

###
#eda %>%
#  select(work_type,smoking_status ,stroke) %>% 
#  ggpairs(mapping = aes(color=stroke))

```




```{r}

eda %>%
  select(hypertension, heart_disease , avg_glucose_level,stroke) %>% 
  ggpairs(mapping = aes(color=stroke))
```





# Statistical Inference
In this section we perform hypothesis test to test whether there are relationship between our predictor variables with the response variable, `stroke`. This will be accomplished in two steps;

i). **chi -square** test of independence between the categorical columns and the stroke i.e.tests the categorical columns are independent/ dependent of the response variable, `is there a relationship between the categorical columns and the Stroke column`.
 Ho: The categorical columns and Stroke are independent
 Ha: The categorical columns and Stroke are dependent
```{r}

chisq.test(table(dff$gender, dff$stroke))[3]

sapply(cat_, function(x){
                    t= table(df$stroke,df[[x]])
                    list(chisq.test(t)$p.value )
                    })
```

From the p-values above; the chances of getting a stroke are dependent on the following factors; `hypertension`, `heart_disease`, `ever_married`, `work_type`, and `smoking_status`.


ii). 

shapiro.test : ho:  vars is normally distributed
               h1: vars is not normally distributed

var.test : ho:  vars are equal
           h1: vars are not equal

t.test: ho: popln means are same.
        h1: popln means are not same.

wilcox.test: ho: The distributions of the two groups are identical
        h1:The distributions of the two groups are different
```{r}

#x = df[ df$stroke==1, "bmi"]
#y = df[ df$stroke==0, "bmi"]
#shapiro.test(df$bmi)
#ks.test(df$bmi, "pnorm", mean = mean(df$bmi), sd = sd(df$bmi))

#var.test(x,y)



#result <- t.test(age ~ stroke, data = dff, alternative="two.sided",   var.equal=F )

#wilcox.test( bmi~ stroke, data = dff)
```

Correlations between Stroke and numeric columns
```{r}
library(corrr)
df %>% correlate() %>%  focus(stroke)

```


# Data Pre-processing
Data Pre-processing steps that will be undertaken are;
- **Train-test split**: splitting the data into train and test sets, in order to carry out model evaluation on unseen / test data and finally picking the best model. This process will be carried out firts in order to avoid data leakage.
- **Categorical Encoding**: Encoding all the categorical columns with dummy variables , to prepare data for modelling.
- **Handling Class Imbalance**: Since our response column `stroke`, has imbalance classes i.e  `r table(df$stroke)`,so as not to create model bais towards the majority class, upsampling of the minority class will be undertaken.

## train test split
We split the data into training(80%) and testing (20%) sets, in order to test model performances on unseen data.
```{r}
# setting seed for re productivity
set.seed(123) 

# train test split
split_ = initial_split(data=df, prop = 0.8, strata= stroke)
train = training(split_)
test = testing(split_)
```

## Categorical encoding
Before modelling we'll have to convert all the categorical columns into integers.
```{r}
# printing the unique values of the categorical columns
sapply(cat_cols, function(x){
                    y=df[[x]]
                    paste( unique(y))
        })
```

Since all the above categories are of Nominal type, no order or ranking, then we are performing **Dummy encoding**.

- **step_dummy()**; in the pre-processing steps will be used to dummy all categorical columns.
```{r}
#library(fastDummies)
#data = dummy_cols(.data= df,
#                 remove_first_dummy = T,   # preventing dummy trap
#                 remove_selected_columns = T)  # removing original columns

```

## handling class imbalance
Handling class imbalance in logistic regression is important to ensure that the model does not disproportionately focus on the majority class and neglect the minority class.

```{r}
#showing class imbalance fo target variable
prop.table(table(data$stroke))
```

Class 0 of stroke,No stroke, contains 95% of the observations while people who had stoke form 5%.

Due to the class imbalance in our target variable, `stroke`, using the `themis::step_smote()` function to perform minority over-sampling until we achieve equal classes in both categories of our `stroke` response column. 

---perform a weighted logistic model, where we assign different weights to the classes during the training phase, with higher weights to the minority class. This way, the model pays more attention to the minority class during training.

Also, in model evaluation, we will choose appropriate evaluation metrics that are not sensitive to class imbalance. For example, use precision, recall, F1-score, or area under the ROC curve (AUC-ROC) instead of accuracy

Building all the above stated pre-processing steps are; 
```{r}

library(recipes)
library(themis)


recipe_ <- recipe(stroke~., data= train) %>% 
  step_string2factor(all_string()) %>% # converting string to factor columns
  step_mutate(stroke= as.factor(stroke)) %>% # converting repsonse variable to factor
  step_dummy(all_nominal(), -all_outcomes()) %>%  # dummying
  step_zv(all_predictors()) %>%       # removing variable with zero variance
  step_normalize(all_predictors()) %>%  # standardizing all predictors
  step_smote(stroke, 
             over_ratio = 1,
             neighbors = 5) %>%      # minority over-sampling
  prep()

# pre-pocessing the steps on train data
train_scaled <- juice(recipe_)

# performing the above steps on test data
test_scaled <- bake(recipe_, new_data = test)
```


# Modelling
Because of the binary nature of our response variable, we will create classifier models and later evaluate their performance on the test / unseen data in order to pick the best model.

Also, due to the nature of our main objective, our metric of interest or of importance is `Recall` since we want to be able to identify people who are at risk of stroke as many as possible, hence high levels of recall.

## Logistic Regression
We will run a weighted logistic regression due to class imbalance and use the models output for inferential purposes, that is;
1. to identify the import features that highly predict stroke chances
2. To identify the odds of each variable to the target variable.



```{r}
library(parsnip)

# creating the logistic model
logit_model = logistic_reg() %>% 
            set_engine("glm") %>% 
            set_mode("classification") %>% 
            fit(stroke~., data= train_scaled)

# evaluating model performance on training data
eval_metrics = metric_set(accuracy, f_meas, recall, precision)

pred_df = logit_model %>% 
            predict(new_data= train_scaled) %>% 
            mutate(Actual = train_scaled$stroke) %>% 
            rename( Predicted = .pred_class)

logit_train_scores = pred_df %>% 
  eval_metrics(truth= Actual, estimate= Predicted, positive="1") %>% 
  select(-.estimator)


logit_train_scores
```

```{r}
# evaluating models performance of test data
pred_pf = logit_model %>% 
            predict(new_data = test_scaled) %>% 
            mutate(Actual = test_scaled$stroke) %>% 
            rename( Predicted = .pred_class)


logit_test_scores = pred_pf %>% 
    eval_metrics(truth= Actual, estimate= Predicted, positive="1") %>% 
  select(-.estimator)


logit_test_scores

```

```{r}
# confusion matrix
pred_pf %>% group_by(Actual, Predicted) %>% count() %>% 
  ggplot(., aes(x=Predicted, y=Actual,fill=n))+
  geom_tile()+
  geom_label(aes(label=n, size=3),color="white")+
  theme(legend.position = "none")
```

From the confusion matrix, we can identify that...




```{r}

# metrics = function( model, data, threshold=0.5){
#   probs = predict(model, newdata = data )
#   y_pred = ifelse( probs>=threshold,1,0)
#   y = data$stroke
#   
#   t = table(y,y_pred)
#   print(t)
#   
#   tn = t[1,1]; tp=t[2,2]; fp= t[1,2]; fn=t[2,1]
#   
#   cat("Boundary: ", threshold,"\n")
#   
#   cat("Precision: ",round(tp/(tp+fp),3),
#       "   Recall: ", round(tp/(tp+fn),3),
#       "\n \n" )
# }
# 
# cat("Training \n")
# cat(metrics(logit_model, train_scaled, 0.4),"\n")
# cat("Testing \n")
# cat(metrics(logit_model, test_scaled,0.4))
```



```{r}
metrics = function( data){

  tn = sum(ifelse(data$Actual=="0" & data$Predicted=="0",1,0))
  tp = sum(ifelse(data$Actual=="1" & data$Predicted=="1",1,0))
  
  fp = sum(ifelse(data$Actual=="0" & data$Predicted=="1",1,0))
  fn = sum(ifelse(data$Actual=="1" & data$Predicted=="0",1,0))
  
  print(table(data$Actual, data$Predicted))
  
  cat("Precision: ",round(tp/(tp+fp),4),
      "   Recall: ", round(tp/(tp+fn),4),
      "\n \n" )
}

#tt= table("Pred"=predict(l_model, train_scaled)$.pred_class,
#          "Actual"=train_scaled$stroke)

cat("Training \n")
cat(metrics(pred_df),"\n")
cat("Testing \n")
cat(metrics(pred_pf))
```



### Logistic Model Hyperparameter tuning
```{r}

tune_logit <- function(x,y){
  model = glm(formula = stroke ~ ., data = train, family = binomial,
              weights = ifelse(stroke==1,x,y))
  
  # validation on test set
  probs =predict(model, newdata=test, type="response")
  y_pred=  ifelse( probs>=0.5,1,0)
  
  t = table(test$stroke,y_pred)
  tn = t[1,1]; tp=t[2,2]; fp= t[1,2]; fn=t[2,1]
  
  return( list("recall" = round(tp/(tp+fn),4),
          "precision" = round(tp/(tp+fp),4)))
}


param_grid <- list(seq(1,20,2), seq(0.5,2,0.3))

results = data.frame("params"=character(), 
                     "recall" = double() ,
                     "precision"=double() )

for(x in param_grid[[1]]){
  for(y in param_grid[[2]])  {
    metrics_ = tune_logit(x,y)
              
    r = data.frame( "params"= paste(x,",",y),
                          "recall" = metrics_$recall,
                          "precision" = metrics_$precision)
    results= rbind(results,r)
              } }

results

```

```{r}
tune_logit <- function(x, y) {
  model <- glm(formula = stroke ~ ., data = train, family = binomial, weights = ifelse(stroke == 1, x, y))
  
  # Validation on the test set
  probs <- predict(model, newdata = test, type = "response")
  y_pred <- ifelse(probs >= 0.5, 1, 0)
  
  t <- table(test$stroke, y_pred)
  tn <- t[1, 1]; tp <- t[2, 2]; fp <- t[1, 2]; fn <- t[2, 1]
  
  recall <- round(tp / (tp + fn), 4)
  precision <- round(tp / (tp + fp), 4)
  
  return(list("recall" = recall, "precision" = precision))
}

param_grid <- list(seq(1, 20, 2), seq(0.5, 0.7,1))

results <- data.frame("params" = character(), 
                      "recall" = double(),
                      "precision" = double())


for (x in param_grid[[1]]) {
  for (y in param_grid[[2]]) {
    metrics_ <- tune_logit(x, y)
    
    r <- data.frame("params" = paste0(x, ",", y),
                    "recall" = metrics_$recall,
                    "precision" = metrics_$precision)
    
    results <- rbind(results, r)
  }
}

print(results)

```

```{r}

results %>% ggplot( aes(x=recall, y = precision, label=params))+
  geom_line()+
  geom_label(size=2)+
  scale_x_continuous(breaks = seq(0, 1, 0.05)) +
  scale_y_continuous(breaks = seq(0, 1, 0.05))
```






## K-Nearest Neigbors Model
```{r}
library(parsnip)
library(yardstick)

knn_model = nearest_neighbor() %>% 
  set_engine("kknn") %>% 
  set_mode("classification") %>% 
  fit(stroke~., data= train_scaled)


# eval_metrics = metric_set(accuracy, f_meas, recall, precision)


pred_df = knn_model %>%
  predict(new_data=train_scaled) %>% 
  mutate(Actual= train_scaled$stroke) %>% 
  rename(Predicted=.pred_class)  

knn_train_scores = pred_df %>% 
  eval_metrics(.,truth=Actual, estimate= Predicted)%>% 
  select(-.estimator) 
  
knn_train_scores

```

```{r}
# testing metrics
pred_pf = knn_model %>%
  predict(new_data=test_scaled) %>% 
  mutate(Actual= test_scaled$stroke) %>% 
  rename(Predicted=.pred_class)  

knn_test_scores = pred_pf %>% 
  eval_metrics(.,truth=Actual, estimate= Predicted)%>% 
  select(-.estimator) 
  
knn_test_scores
```

```{r}
# confusion matrix
pred_pf %>% group_by(Actual, Predicted) %>% count() %>% 
  ggplot(., aes(x=Predicted, y=Actual,fill=n))+
  geom_tile()+
  geom_label(aes(label=n, size=3),color="white")+
  theme(legend.position = "none")
```


```{r}
#validation 2

cat("Training \n")
cat(metrics(pred_df),"\n")
cat("Testing \n")
cat(metrics(pred_pf))
```




## Decision Tree Model
### Method 1
```{r}

# train_ = train
# train_$stroke = as.factor(train_$stroke)


t_model = decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification") %>% 
  fit(stroke~., data= train_scaled)

pred_df = t_model %>%
  predict(new_data=train_scaled) %>% 
  mutate(Actual= train_scaled$stroke) %>% 
  rename(Predicted=.pred_class)  

tree_train_scores = pred_df %>% 
  eval_metrics(.,truth=Actual, estimate= Predicted, positive="1")%>% 
  select(-.estimator) 
  
tree_train_scores

```


```{r}

# test_ =test
# test_$stroke = as.factor(test_$stroke)

# testing metrics
pred_pf = t_model %>%
  predict(test_scaled) %>% 
  mutate(Actual= test_scaled$stroke) %>% 
  rename(Predicted=.pred_class)  #

tree_test_scores = pred_pf %>% 
  eval_metrics(.,truth = Actual, estimate= Predicted)%>% 
  select(-.estimator) 
  
tree_test_scores

```

```{r}
# confusion matrix
pred_pf %>% group_by(Actual, Predicted) %>% count() %>% 
  ggplot(., aes(x=Predicted, y=Actual,fill=n))+
  geom_tile()+
  geom_label(aes(label=n, size=3),color="white")+
  theme(legend.position = "none")
```


```{r}
cat("Training \n")
cat(metrics(pred_df),"\n")
cat("Testing \n")
cat(metrics(pred_pf))
```


### Method 2
```{r}
library(rpart)
library(rpart.plot)

# building tree model
tree_model = rpart(formula = stroke~.,
                   data=train,
                   control =  rpart.control(cp=0.01))

# plotting the tree model
rpart.plot(tree_model, box.palette = "RdBu", digits = -3)
```


**cp:complexity parameter**: is a pruning parameter that penalizing the addition of nodes that do not contribute enough to the overall improvement in the model's performance. ie controls the growth of a tree

```{r}
# model evaluation
metric = function( model, df,boundary ){
  probs = predict(model, df)
  y_pred= ifelse(probs>= boundary, 1,0)
  y = df$stroke

  t = table(y,y_pred)
  print(t)
  tn = t[1,1]; tp=t[2,2]; fp= t[1,2]; fn=t[2,1]
  
  cat("Boundary: ", boundary,"\n")
  
  cat("Accuracy: ",round((tp+tn)/(tp+tn+fp+fn),4) ,
      "  Precision: ",round(tp/(tp+fp),4),
      "   Recall: ", round(tp/(tp+fn),4),
      "\n \n" )
}

cat("Training \n")
cat(metric(tree_model, train, 0.05),"\n")
cat("Testing \n")
cat(metric(tree_model, test,0.05))

```

```{r}
y_train_pred = ifelse( predict(tree_model, train)>=0.1,1,0) #default type is vector
y_test_pred = ifelse( predict(tree_model, test)>=0.05,1,0)

eval_metrics = metric_set( accuracy, f_meas, recall, precision)

eval_metrics(data = tibble("stroke"= as.factor(train$stroke),
                     "prediction"=as.factor(y_train_pred)),
       truth=stroke, 
       estimate= prediction)
```

#### Tree Pruning
```{r}
printcp(tree_model)
plotcp(tree_model)

index = which.min(tree_model$cptable[,"xerror"]) #index of the min cp
min_cp = tree_model$cptable[index,"CP"] # value of minimum cp value
# tree creation
tree_prunned = rpart(formula = stroke~.,
                  data= train,
                  control = rpart.control(minsplit = 20,
                                          maxdepth = 4,
                                          cp=0))
# plotting tree
rpart.plot(tree_prunned, box.palette="RdBu", digits=-3)
```

```{r}
cat("Training \n")
cat(metric(tree_prunned, train, 0.5),"\n")
cat("Testing \n")
cat(metric(tree_prunned, test,0.5))
```






## Random Forests

```{r}
# train = train %>% rename(work_type_Self_employed=`work_type_Self-employed`,
#                          smoking_status_never_smoked=`smoking_status_never smoked`)
# test =test %>% rename(work_type_Self_employed=`work_type_Self-employed`,
#                       smoking_status_never_smoked=`smoking_status_never smoked`)
# 
# 
# train$stroke = as.factor(train$stroke)
# test$stroke = as.factor(test$stroke)
```

```{r}
set.seed(123)
library(randomForest)

# random forest model
rf_model = randomForest(formula = stroke~.,
                        data = train_scaled,
                        ntrees=500)

```

```{r}
# model evaluation
# train scores
y_train_pred = predict(rf_model, train_scaled)
y_test_pred = predict(rf_model, test_scaled)

eval_metrics(data=tibble("Actual"=train_scaled$stroke,
                            "Predicted"= y_train_pred), 
             truth=Actual,
             estimate= Predicted )
```

```{r}
# test scores
eval_metrics(data=tibble("Actual"=test_scaled$stroke,
                            "Predicted"= y_test_pred), 
             truth=Actual,
             estimate= Predicted )
```




```{r}
metric_ = function( model, data){
  y_pred = predict(model, newdata = data )
 
  y = data$stroke
  
  t = table(y,y_pred)
  print(t)
  
  tn = t[1,1]; tp=t[2,2]; fp= t[1,2]; fn=t[2,1]

  
  cat("Precision: ",round(tp/(tp+fp),3),
      "   Recall: ", round(tp/(tp+fn),3),
      "\n \n" )
}




cat("Training \n")
cat(metric_(rf_model, train_scaled),"\n")
cat("Testing \n")
cat(metric_(rf_model, test_scaled))

```

# Conclusion


# Recommendations


















